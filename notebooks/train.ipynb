{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9P9PmZMoK9Y"
      },
      "source": [
        "# 概要\n",
        "歌詞からアーティスト名を予測する分類モデルを学習させるコードです。\n",
        "\n",
        "テキストを文字単位で分割して入力する「CharacterCNN」を採用しています。\n",
        "\n",
        "論文→https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL-N1Gwz_VZ3"
      },
      "source": [
        "まず全アーティスト分の予測を行うネットワークを学習させます（事前学習）。\n",
        "\n",
        "その重みを初期値として、最後の全結合層の部分のみを取り換え、各アーティストについて2値分類モデル（そのアーティストの歌詞かそうでないか）を学習させます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpaNulaWo9Ui"
      },
      "source": [
        "# ライブラリのインポート/クラスや関数の定義/歌詞データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OqkgxvoXDYk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xWTUZ_8pCJY"
      },
      "outputs": [],
      "source": [
        "# 保存先のパス\n",
        "PATH = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_fld_QFX0TH"
      },
      "outputs": [],
      "source": [
        "def encode(txt, max_length=200):\n",
        "    \"\"\"\n",
        "    歌詞の1文字1文字をUnicodeに変換する関数\n",
        "\n",
        "    Parameters:\n",
        "        txt (iterable of str): Text or iterable of texts to be encoded.\n",
        "        max_length (int, optional): Maximum length of encoded sequence. Default is 200.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Encoded sequence(s) as a NumPy array.\n",
        "\n",
        "    Notes:\n",
        "        - Each character in the input text(s) is converted to its corresponding Unicode code point.\n",
        "        - The resulting encoded sequence(s) are padded or truncated to match the specified maximum length.\n",
        "        - If the input text(s) are shorter than the maximum length, the remaining elements are filled with zeros.\n",
        "\n",
        "    Example:\n",
        "        txt = [\"Hello, world!\"]\n",
        "        encoded_txt = encode(txt, max_length=10)\n",
        "        print(encoded_txt)\n",
        "        # Output: [[ 72 101 108 108 111  44  32 119 111 114]]\n",
        "    \"\"\"\n",
        "    txt_list = []\n",
        "    for l in txt:\n",
        "        txt_line = [ord(x) for x in str(l).strip()]\n",
        "        txt_line = txt_line[:max_length]\n",
        "        txt_len = len(txt_line)\n",
        "        if txt_len < max_length:\n",
        "            txt_line += ([0] * (max_length - txt_len))\n",
        "        txt_list.append((txt_line))\n",
        "    return np.array(txt_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNhL2V6Ym-1B"
      },
      "outputs": [],
      "source": [
        "class CharacterCNN(nn.Module):\n",
        "    def __init__(self, num_classes, embed_size=256, filter_sizes=(2, 3, 4, 5), filter_num=64):\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.filter_num = filter_num\n",
        "\n",
        "        self.embedding = nn.Embedding(0xffff, embed_size)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(embed_size, filter_num, filter_size) for filter_size in filter_sizes\n",
        "        ])\n",
        "        self.fc1 = nn.Linear(filter_num * len(filter_sizes), 64)\n",
        "        self.batch_norm = nn.BatchNorm1d(64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x).transpose(1,2)\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv_layer in self.conv_layers:\n",
        "            conv_output = F.relu(conv_layer(embedded))\n",
        "            pooled = F.max_pool1d(conv_output, conv_output.size(2)).squeeze(2)\n",
        "            conv_outputs.append(pooled)\n",
        "\n",
        "        convs_merged = torch.cat(conv_outputs, dim=1)\n",
        "        fc1_output = F.relu(self.fc1(convs_merged))\n",
        "        bn_output = self.batch_norm(fc1_output)\n",
        "        do_output = self.dropout(bn_output)\n",
        "        fc2_output = self.fc2(do_output)\n",
        "        return fc2_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwObNFdyB4JR"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, n_epochs, batch_size, learning_rate, criterion, gkf, groups, pretrain, device):\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.criterion = criterion\n",
        "        self.gkf = gkf\n",
        "        self.groups = groups\n",
        "        self.pretrain = pretrain\n",
        "        self.n_splits = gkf.get_n_splits()\n",
        "        self.bst_model, self.bst_score = dict(), dict()\n",
        "\n",
        "    def set_model(self, network, state_dict):\n",
        "        self.network = network\n",
        "        self.state_dict = copy.deepcopy(state_dict)\n",
        "\n",
        "    def reset_model(self):\n",
        "        model = self.network\n",
        "        model.load_state_dict(self.state_dict)\n",
        "        return model\n",
        "\n",
        "    def train(self, X, y, verbose=1):\n",
        "        for fold, (tr_idx, va_idx) in enumerate(self.gkf.split(X, y, groups=self.groups)):\n",
        "            # 学習データと評価用データに分割\n",
        "            print(f'-----Fold{fold+1}/{self.n_splits}-----')\n",
        "            X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "            y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "            n_iter = len(y_tr) // self.batch_size\n",
        "\n",
        "            model = self.reset_model().to(self.device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "            self.bst_model[fold] = None\n",
        "            self.bst_score[fold] = -np.inf\n",
        "\n",
        "            # 学習\n",
        "            for epoch in range(self.n_epochs):\n",
        "                model.train()\n",
        "                total_loss = 0.0\n",
        "                total_correct = 0\n",
        "                random_idx = np.random.permutation(len(y_tr))\n",
        "                for i in range(n_iter):\n",
        "                    X_batch = torch.from_numpy(X_tr[random_idx[self.batch_size*i:self.batch_size*(i+1)]]).to(self.device)\n",
        "                    y_batch = torch.from_numpy(y_tr[random_idx[self.batch_size*i:self.batch_size*(i+1)]]).to(self.device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(X_batch)\n",
        "                    loss = self.criterion(outputs, y_batch)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = outputs.max(dim=1)\n",
        "                    total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "                train_loss = total_loss / n_iter\n",
        "                train_acc = total_correct / (self.batch_size*n_iter)\n",
        "\n",
        "                # 評価\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(torch.from_numpy(X_va).to(self.device))\n",
        "                    loss = criterion(outputs, torch.from_numpy(y_va).to(self.device))\n",
        "                    valid_loss = loss.item()\n",
        "                    _, predicted = outputs.max(dim=1)\n",
        "                    valid_acc = accuracy_score(y_va, predicted.cpu().numpy())\n",
        "                    valid_f1 = f1_score(y_va, predicted.cpu().numpy(), average='macro')\n",
        "                if (epoch+1) % verbose == 0:\n",
        "                    print(f'Epoch[{epoch+1}/{self.n_epochs}], TrainLoss: {train_loss:.4f}, ValidLoss: {valid_loss:.4f}, TrainAcc: {train_acc*100:.4f}%, ValidAcc: {valid_acc*100:.4f}%, ValidF1: {valid_f1:.4f}')\n",
        "\n",
        "                # best更新処理\n",
        "                if valid_f1 > self.bst_score[fold]:\n",
        "                    self.bst_model[fold] = copy.deepcopy(model)\n",
        "                    self.bst_score[fold] = valid_f1\n",
        "\n",
        "            if self.pretrain:\n",
        "                break\n",
        "\n",
        "    def save_all(self, dirname):\n",
        "        os.makedirs(dirname, exist_ok=True)\n",
        "        for fold, model in self.bst_model.items():\n",
        "            filepath = dirname + '/' + f'model_fold{fold+1}.pth'\n",
        "            torch.save(model.state_dict(), filepath)\n",
        "            print(f'[Saved] score:{self.bst_score[fold]:.4f}  @ {filepath}')\n",
        "\n",
        "    def save_clf(self, dirname):\n",
        "        os.makedirs(dirname, exist_ok=True)\n",
        "        for fold, model in self.bst_model.items():\n",
        "            filepath = dirname + '/' + f'classifier_fold{fold+1}.pth'\n",
        "            clf_s = OrderedDict(list(model.state_dict().items())[1:])\n",
        "            torch.save(clf_s, filepath)\n",
        "            print(f'[Saved] score:{self.bst_score[fold]:.4f}  @ {filepath}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwqsUzxX6P0-",
        "outputId": "3ef9d7fd-0877-4da3-ff61-1355ab9fa8a4"
      },
      "outputs": [],
      "source": [
        "# 歌詞CSVの読み込み\n",
        "df = pd.read_csv(PATH + '/data/lyric_block.csv')\n",
        "print(df.artist.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1efaXtziHdt"
      },
      "outputs": [],
      "source": [
        "# 分類先アーティスト\n",
        "artists = list(df.artist.unique())\n",
        "artist_to_label = {artist:i for i,artist in enumerate(artists)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCOANX_AOHt"
      },
      "source": [
        "# 事前学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ_sMNk8MsHc"
      },
      "source": [
        "## 前処理\n",
        "- 各文字をUnicodeに変換\n",
        "- 特徴量をXへ、ラベルをyへ格納"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bESv1WB4Aw6z",
        "outputId": "45fe17fe-2378-4d17-c2cc-42584d659062"
      },
      "outputs": [],
      "source": [
        "# 説明変数X\n",
        "X = encode(df['block'])\n",
        "# 目的変数y\n",
        "y = df['artist'].map(artist_to_label).values\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvZu3dJMx9M"
      },
      "source": [
        "## 学習\n",
        "- `n_epochs`, `batch_size`, `learning_rate`, `optimizer`などを設定する。\n",
        "- 同じ歌の歌詞が、学習用データと評価用データへ混在しないようにGroupKFold。（1曲の中で同じ歌詞が繰り返し登場するため）\n",
        "- 事前学習では、1fold目の結果を用いる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPr22VitBESG",
        "outputId": "7f38b69a-89e2-4196-bc6d-510a1b524dfc"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20          # エポック数\n",
        "batch_size = 512       # バッチサイズ\n",
        "learning_rate = 0.001  # 学習率\n",
        "n_splits = 5           # GroupKFoldの分割数\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "gkf = GroupKFold(n_splits,)  # GroupKfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00ynctb0mLS2"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjrVcMieFedw",
        "outputId": "a10929ad-7e9e-4d59-8633-bc789230086a"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "model = CharacterCNN(num_classes=len(artists), embed_size=256, filter_sizes=(2,3,4,5), filter_num=64)\n",
        "\n",
        "pretrain = Trainer(n_epochs, batch_size, learning_rate, criterion, gkf, df['title'], True, device)\n",
        "pretrain.set_model(model, model.state_dict())\n",
        "pretrain.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnsyhy4HmVt4",
        "outputId": "18f0adaa-4c80-4a0d-d9ba-905b66ef23dd"
      },
      "outputs": [],
      "source": [
        "pretrain.save_all(PATH + '/pretrain')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD5p3grIM-BD"
      },
      "source": [
        "# 事前学習モデルをファインチューニング\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIO2AlvuOBBU"
      },
      "outputs": [],
      "source": [
        "# 事前学習重みを読み込む\n",
        "weight_path = PATH + '/pretrain/model_fold1.pth'\n",
        "model = CharacterCNN(num_classes=len(artists), embed_size=256, filter_sizes=(2,3,4,5), filter_num=64)\n",
        "model.load_state_dict(torch.load(weight_path))\n",
        "\n",
        "# Embeddingのみweightを固定\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    break\n",
        "\n",
        "# 出力層を変更\n",
        "model.fc2 = nn.Linear(64, 2, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Msf1Fq_B1e",
        "outputId": "75661b61-83ce-4584-c041-3def4cf9a1f9"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20          # エポック数\n",
        "batch_size = 128       # バッチサイズ\n",
        "learning_rate = 0.001  # 学習率\n",
        "n_splits = 5           # GroupKFoldの分割数\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "gkf = GroupKFold(n_splits)  # GroupKfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKiHbhXINJcA",
        "outputId": "37117fe4-132d-434f-d321-c3070d7e4717"
      },
      "outputs": [],
      "source": [
        "for artist in artists:\n",
        "    df_artist = df[df['artist'] == artist]\n",
        "    df_other = df[df['artist'] != artist].sample(len(df_artist))\n",
        "    df_sub = pd.concat([df_artist, df_other])\n",
        "\n",
        "    X = encode(df_sub['block'])\n",
        "    y = df_sub['artist'].map({artist:1}).fillna(0).values.astype('int')\n",
        "\n",
        "    train = Trainer(n_epochs, batch_size, learning_rate, criterion, gkf, df_sub['title'], False, device)\n",
        "    train.set_model(model, model.state_dict())\n",
        "    train.train(X, y, 10)\n",
        "    train.save_clf(PATH + f'/models/{artist}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
